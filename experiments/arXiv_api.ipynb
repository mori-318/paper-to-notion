{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db304014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<feed xmlns=\"http://www.w3.org/2005/Atom\">\n",
      "  <link href=\"http://arxiv.org/api/query?search_query%3Dti%3A%22language%20model%22%20AND%20abs%3A%22distillation%22%26id_list%3D%26start%3D0%26max_results%3D5\" rel=\"self\" type=\"application/atom+xml\"/>\n",
      "  <title type=\"html\">ArXiv Query: search_query=ti:\"language model\" AND abs:\"distillation\"&amp;id_list=&amp;start=0&amp;max_results=5</title>\n",
      "  <id>http://arxiv.org/api/TfsaVs9T7Qi+bmYuluq9IETuM+Q</id>\n",
      "  <updated>2025-09-19T00:00:00-04:00</updated>\n",
      "  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">759</opensearch:totalResults>\n",
      "  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\n",
      "  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">5</opensearch:itemsPerPage>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2509.14930v1</id>\n",
      "    <updated>2025-09-18T13:07:53Z</updated>\n",
      "    <published>2025-09-18T13:07:53Z</published>\n",
      "    <title>Cross-Modal Knowledge Distillation for Speech Large Language Models</title>\n",
      "    <summary>  In this work, we present the first systematic evaluation of catastrophic\n",
      "forgetting and modality inequivalence in speech large language models, showing\n",
      "that introducing speech capabilities can degrade knowledge and reasoning even\n",
      "when inputs remain textual, and performance further decreases with spoken\n",
      "queries. To address these challenges, we propose a cross-modal knowledge\n",
      "distillation framework that leverages both text-to-text and speech-to-text\n",
      "channels to transfer knowledge from a text-based teacher model to a speech LLM.\n",
      "Extensive experiments on dialogue and audio understanding tasks validate the\n",
      "effectiveness of our approach in preserving textual knowledge, improving\n",
      "cross-modal alignment, and enhancing reasoning in speech-based interactions.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Enzhi Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Qicheng Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhiyuan Tang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yuhang Jia</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/2509.14930v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2509.14930v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2509.14526v1</id>\n",
      "    <updated>2025-09-18T01:42:24Z</updated>\n",
      "    <published>2025-09-18T01:42:24Z</published>\n",
      "    <title>Delta Knowledge Distillation for Large Language Models</title>\n",
      "    <summary>  Knowledge distillation (KD) is a widely adopted approach for compressing\n",
      "large neural networks by transferring knowledge from a large teacher model to a\n",
      "smaller student model. In the context of large language models, token level KD,\n",
      "typically minimizing the KL divergence between student output distribution and\n",
      "teacher output distribution, has shown strong empirical performance. However,\n",
      "prior work assumes student output distribution and teacher output distribution\n",
      "share the same optimal representation space, a premise that may not hold in\n",
      "many cases. To solve this problem, we propose Delta Knowledge Distillation\n",
      "(Delta-KD), a novel extension of token level KD that encourages the student to\n",
      "approximate an optimal representation space by explicitly preserving the\n",
      "distributional shift Delta introduced during the teacher's supervised\n",
      "finetuning (SFT). Empirical results on ROUGE metrics demonstrate that Delta KD\n",
      "substantially improves student performance while preserving more of the\n",
      "teacher's knowledge.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Yihan Cao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yanbin Kang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhengming Xing</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ruijie Jiang</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8 pages, 3 figures</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/2509.14526v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2509.14526v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2509.13785v1</id>\n",
      "    <updated>2025-09-17T07:55:39Z</updated>\n",
      "    <published>2025-09-17T07:55:39Z</published>\n",
      "    <title>Summary on The Multilingual Conversational Speech Language Model\n",
      "  Challenge: Datasets, Tasks, Baselines, and Methods</title>\n",
      "    <summary>  This paper summarizes the Interspeech2025 Multilingual Conversational Speech\n",
      "Language Model (MLC-SLM) challenge, which aims to advance the exploration of\n",
      "building effective multilingual conversational speech LLMs (SLLMs). We provide\n",
      "a detailed description of the task settings for the MLC-SLM challenge, the\n",
      "released real-world multilingual conversational speech dataset totaling\n",
      "approximately 1,604 hours, and the baseline systems for participants. The\n",
      "MLC-SLM challenge attracts 78 teams from 13 countries to participate, with 489\n",
      "valid leaderboard results and 14 technical reports for the two tasks. We\n",
      "distill valuable insights on building multilingual conversational SLLMs based\n",
      "on submissions from participants, aiming to contribute to the advancement of\n",
      "the community.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Bingshen Mu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Pengcheng Guo</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhaokai Sun</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shuai Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hexin Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mingchen Shao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Lei Xie</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Eng Siong Chng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Longshuai Xiao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Qiangze Feng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Daliang Wang</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/2509.13785v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2509.13785v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2509.11815v1</id>\n",
      "    <updated>2025-09-15T11:53:56Z</updated>\n",
      "    <published>2025-09-15T11:53:56Z</published>\n",
      "    <title>SpecVLM: Fast Speculative Decoding in Vision-Language Models</title>\n",
      "    <summary>  Speculative decoding is a powerful way to accelerate autoregressive large\n",
      "language models (LLMs), but directly porting it to vision-language models\n",
      "(VLMs) faces unique systems constraints: the prefill stage is dominated by\n",
      "visual tokens whose count scales with image resolution and video length,\n",
      "inflating both compute and memory, especially the key-value (KV) cache. We\n",
      "study speculative decoding for VLMs and introduce SpecVLM, a practical system\n",
      "that (1) establishes a strong EAGLE-2-style baseline, EagleVLM, delivering\n",
      "1.5--2.3x end-to-end speedups over full autoregressive inference, and (2)\n",
      "further accelerates VLM inference with an elastic visual compressor that\n",
      "adaptively selects among pruning, pooling, convolution, and resampler\n",
      "primitives to balance FLOPs/parameters and accuracy per input. To avoid costly\n",
      "offline distillation corpora, we propose an online-logit distillation protocol\n",
      "that trains the draft model with on-the-fly teacher logits and penultimate\n",
      "features using a combined cross-entropy and Smooth L1 objective, eliminating\n",
      "storage and preprocessing while remaining compute-efficient. This protocol\n",
      "reveals a training-time scaling effect: longer online training monotonically\n",
      "increases the draft model's average accepted length, improving speculative\n",
      "efficiency. Empirically, SpecVLM achieves additional acceleration, culminating\n",
      "in 2.5--2.9x end-to-end speedups within 5 epochs across LLaVA and MMMU,\n",
      "consistently over resolutions and task difficulties, while preserving the\n",
      "target model's output distribution (lossless decoding). Our code is available\n",
      "at https://github.com/haiduo/SpecVLM.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Haiduo Huang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Fuwei Yang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhenhua Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xuanwu Yin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Dong Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Pengju Ren</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Emad Barsoum</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/2509.11815v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2509.11815v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2509.11575v1</id>\n",
      "    <updated>2025-09-15T04:39:50Z</updated>\n",
      "    <published>2025-09-15T04:39:50Z</published>\n",
      "    <title>A Survey of Reasoning and Agentic Systems in Time Series with Large\n",
      "  Language Models</title>\n",
      "    <summary>  Time series reasoning treats time as a first-class axis and incorporates\n",
      "intermediate evidence directly into the answer. This survey defines the problem\n",
      "and organizes the literature by reasoning topology with three families: direct\n",
      "reasoning in one step, linear chain reasoning with explicit intermediates, and\n",
      "branch-structured reasoning that explores, revises, and aggregates. The\n",
      "topology is crossed with the main objectives of the field, including\n",
      "traditional time series analysis, explanation and understanding, causal\n",
      "inference and decision making, and time series generation, while a compact tag\n",
      "set spans these axes and captures decomposition and verification, ensembling,\n",
      "tool use, knowledge access, multimodality, agent loops, and LLM alignment\n",
      "regimes. Methods and systems are reviewed across domains, showing what each\n",
      "topology enables and where it breaks down in faithfulness or robustness, along\n",
      "with curated datasets, benchmarks, and resources that support study and\n",
      "deployment (https://github.com/blacksnail789521/Time-Series-Reasoning-Survey).\n",
      "Evaluation practices that keep evidence visible and temporally aligned are\n",
      "highlighted, and guidance is distilled on matching topology to uncertainty,\n",
      "grounding with observable artifacts, planning for shift and streaming, and\n",
      "treating cost and latency as design budgets. We emphasize that reasoning\n",
      "structures must balance capacity for grounding and self-correction against\n",
      "computational cost and reproducibility, while future progress will likely\n",
      "depend on benchmarks that tie reasoning quality to utility and on closed-loop\n",
      "testbeds that trade off cost and risk under shift-aware, streaming, and\n",
      "long-horizon settings. Taken together, these directions mark a shift from\n",
      "narrow accuracy toward reliability at scale, enabling systems that not only\n",
      "analyze but also understand, explain, and act on dynamic worlds with traceable\n",
      "evidence and credible outcomes.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Ching Chang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yidan Shi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Defu Cao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Wei Yang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jeehyun Hwang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Haixin Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jiacheng Pang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Wei Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yan Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Wen-Chih Peng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tien-Fu Chen</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">This paper is currently under review</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/2509.11575v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2509.11575v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "</feed>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "base_url = \"http://export.arxiv.org/api/query\"\n",
    "params = {\n",
    "    \"search_query\": 'ti:\"language model\" AND abs:\"distillation\"',\n",
    "    \"sortBy\": \"submittedDate\",\n",
    "    \"sortOrder\": \"descending\",\n",
    "    \"max_results\": 5\n",
    "}\n",
    "\n",
    "response = requests.get(base_url, params=params)\n",
    "print(response.text)  # XMLレスポンス\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
