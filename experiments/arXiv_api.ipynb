{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0dba60b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我々はDeepSeek-V3を紹介します。671億のパラメータの強力な混合型エキスパート言語モデルで、3億7100万のパラメータがアクティブ化されています。効率的な推論とコスト効果の高いトレーニングを目的としています。Multi-Head Latent Attention（MLA）やDeepSeekMoEアーキ構造を採用し、これらの構造はDeepSeek-V2において徹底的に検証を受けています。さらにDeepSeek-V3は、ロードバランスに有効なサポート補助損失戦略を持ち、より良い性能を実現するための多トークン予測訓練の目標設定をします。1480億365万という多様な高品質なトークンをプレトレーニングして、次にスーパービルドなFineTuningとReinforcement Learningによってその能力を最大限に使用しています。検証は、DeepSeek-V3がオープンソースモデルより優れており高い性能を実現しました。優れた性能を持つ一方で、DeepSeek-V3はH800 GPUの2,788万時間でフルトレーニングできます。さらには、トレーニングプロセスも非常に安定しています。トレーニング中の損失の irrecoverable への Spike に遭遇したり、ロールバックがあったりなどありません。このモデルcheckpointは https にあるURLでご覧になれます\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1',\n",
    "    api_key='ollama' # 必須だが値は使用されない\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"7shi/gemma-2-jpn-translate:2b-instruct-q8_0\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"入力された文章を日本語に翻訳してください。\"},\n",
    "        {\"role\": \"user\", \"content\": \"We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. The model checkpoints are available at this https URL.\"}\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d909b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "私はGoogleによってトレーニングされた、大規模言語モデルです。\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client(api_key=\"AIzaSyAq1onVUTumDtJLvRSB9WERi2gM_CKBAT4\")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"あなたは誰？モデル名は？\",\n",
    ")\n",
    "\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e3004fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_load_from_file_impl: using device Metal (Apple M3) - 10916 MiB free\n",
      "llama_model_loader: loaded meta data with 38 key-value pairs and 288 tensors from /Users/mori/.cache/huggingface/hub/models--webbigdata--gemma-2-2b-jpn-it-translate-gguf/snapshots/e9c0c0452c774b1528768ffe0980968f116196dc/./gemma-2-2b-jpn-it-translate-IQ3_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Gemma 2 2b Jpn It\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Google\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = jpn-it\n",
      "llama_model_loader: - kv   5:                           general.basename str              = gemma-2\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 2B\n",
      "llama_model_loader: - kv   7:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   8:                    gemma2.embedding_length u32              = 2304\n",
      "llama_model_loader: - kv   9:                         gemma2.block_count u32              = 26\n",
      "llama_model_loader: - kv  10:                 gemma2.feed_forward_length u32              = 9216\n",
      "llama_model_loader: - kv  11:                gemma2.attention.head_count u32              = 8\n",
      "llama_model_loader: - kv  12:             gemma2.attention.head_count_kv u32              = 4\n",
      "llama_model_loader: - kv  13:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  14:                gemma2.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv  15:              gemma2.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv  16:                          general.file_type u32              = 27\n",
      "llama_model_loader: - kv  17:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  18:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  19:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,256000]  = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  30:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  32:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  33:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  34:                      quantize.imatrix.file str              = imatrix.dat\n",
      "llama_model_loader: - kv  35:                   quantize.imatrix.dataset str              = .\\calibration_datav3_plus_jpn_v1.txt\n",
      "llama_model_loader: - kv  36:             quantize.imatrix.entries_count i32              = 182\n",
      "llama_model_loader: - kv  37:              quantize.imatrix.chunks_count i32              = 636\n",
      "llama_model_loader: - type  f32:  105 tensors\n",
      "llama_model_loader: - type q4_K:   55 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llama_model_loader: - type iq3_s:  127 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = IQ3_S mix - 3.66 bpw\n",
      "print_info: file size   = 1.29 GiB (4.25 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:     43 '<unused36>' is not marked as EOG\n",
      "load: control token:    106 '<start_of_turn>' is not marked as EOG\n",
      "load: control token:     26 '<unused19>' is not marked as EOG\n",
      "load: control token:     80 '<unused73>' is not marked as EOG\n",
      "load: control token:     57 '<unused50>' is not marked as EOG\n",
      "load: control token:     71 '<unused64>' is not marked as EOG\n",
      "load: control token:     64 '<unused57>' is not marked as EOG\n",
      "load: control token:     21 '<unused14>' is not marked as EOG\n",
      "load: control token:     95 '<unused88>' is not marked as EOG\n",
      "load: control token:     54 '<unused47>' is not marked as EOG\n",
      "load: control token:     23 '<unused16>' is not marked as EOG\n",
      "load: control token:     48 '<unused41>' is not marked as EOG\n",
      "load: control token:     81 '<unused74>' is not marked as EOG\n",
      "load: control token:     82 '<unused75>' is not marked as EOG\n",
      "load: control token:     46 '<unused39>' is not marked as EOG\n",
      "load: control token:     90 '<unused83>' is not marked as EOG\n",
      "load: control token:     76 '<unused69>' is not marked as EOG\n",
      "load: control token:     78 '<unused71>' is not marked as EOG\n",
      "load: control token:     92 '<unused85>' is not marked as EOG\n",
      "load: control token:     44 '<unused37>' is not marked as EOG\n",
      "load: control token:     16 '<unused9>' is not marked as EOG\n",
      "load: control token:     49 '<unused42>' is not marked as EOG\n",
      "load: control token:     35 '<unused28>' is not marked as EOG\n",
      "load: control token:     56 '<unused49>' is not marked as EOG\n",
      "load: control token:     17 '<unused10>' is not marked as EOG\n",
      "load: control token:     74 '<unused67>' is not marked as EOG\n",
      "load: control token:    105 '<unused98>' is not marked as EOG\n",
      "load: control token:     13 '<unused6>' is not marked as EOG\n",
      "load: control token:     93 '<unused86>' is not marked as EOG\n",
      "load: control token:     37 '<unused30>' is not marked as EOG\n",
      "load: control token:     75 '<unused68>' is not marked as EOG\n",
      "load: control token:     40 '<unused33>' is not marked as EOG\n",
      "load: control token:     38 '<unused31>' is not marked as EOG\n",
      "load: control token:     97 '<unused90>' is not marked as EOG\n",
      "load: control token:     20 '<unused13>' is not marked as EOG\n",
      "load: control token:     28 '<unused21>' is not marked as EOG\n",
      "load: control token:     89 '<unused82>' is not marked as EOG\n",
      "load: control token:    100 '<unused93>' is not marked as EOG\n",
      "load: control token:    102 '<unused95>' is not marked as EOG\n",
      "load: control token:      9 '<unused2>' is not marked as EOG\n",
      "load: control token:     94 '<unused87>' is not marked as EOG\n",
      "load: control token:     86 '<unused79>' is not marked as EOG\n",
      "load: control token:     53 '<unused46>' is not marked as EOG\n",
      "load: control token:    103 '<unused96>' is not marked as EOG\n",
      "load: control token:      6 '[@BOS@]' is not marked as EOG\n",
      "load: control token:     18 '<unused11>' is not marked as EOG\n",
      "load: control token:     11 '<unused4>' is not marked as EOG\n",
      "load: control token:     12 '<unused5>' is not marked as EOG\n",
      "load: control token:     33 '<unused26>' is not marked as EOG\n",
      "load: control token:     60 '<unused53>' is not marked as EOG\n",
      "load: control token:     65 '<unused58>' is not marked as EOG\n",
      "load: control token:     34 '<unused27>' is not marked as EOG\n",
      "load: control token:     63 '<unused56>' is not marked as EOG\n",
      "load: control token:     91 '<unused84>' is not marked as EOG\n",
      "load: control token:     77 '<unused70>' is not marked as EOG\n",
      "load: control token:     96 '<unused89>' is not marked as EOG\n",
      "load: control token:     62 '<unused55>' is not marked as EOG\n",
      "load: control token:      8 '<unused1>' is not marked as EOG\n",
      "load: control token:     52 '<unused45>' is not marked as EOG\n",
      "load: control token:     99 '<unused92>' is not marked as EOG\n",
      "load: control token:     39 '<unused32>' is not marked as EOG\n",
      "load: control token:     87 '<unused80>' is not marked as EOG\n",
      "load: control token:     70 '<unused63>' is not marked as EOG\n",
      "load: control token:     68 '<unused61>' is not marked as EOG\n",
      "load: control token:     32 '<unused25>' is not marked as EOG\n",
      "load: control token:     85 '<unused78>' is not marked as EOG\n",
      "load: control token:    104 '<unused97>' is not marked as EOG\n",
      "load: control token:     79 '<unused72>' is not marked as EOG\n",
      "load: control token:     42 '<unused35>' is not marked as EOG\n",
      "load: control token:     84 '<unused77>' is not marked as EOG\n",
      "load: control token:     55 '<unused48>' is not marked as EOG\n",
      "load: control token:     10 '<unused3>' is not marked as EOG\n",
      "load: control token:     30 '<unused23>' is not marked as EOG\n",
      "load: control token:     59 '<unused52>' is not marked as EOG\n",
      "load: control token:      5 '<2mass>' is not marked as EOG\n",
      "load: control token:     50 '<unused43>' is not marked as EOG\n",
      "load: control token:     15 '<unused8>' is not marked as EOG\n",
      "load: control token:     73 '<unused66>' is not marked as EOG\n",
      "load: control token:     47 '<unused40>' is not marked as EOG\n",
      "load: control token:     19 '<unused12>' is not marked as EOG\n",
      "load: control token:     69 '<unused62>' is not marked as EOG\n",
      "load: control token:      1 '<eos>' is not marked as EOG\n",
      "load: control token:     24 '<unused17>' is not marked as EOG\n",
      "load: control token:     51 '<unused44>' is not marked as EOG\n",
      "load: control token:     41 '<unused34>' is not marked as EOG\n",
      "load: control token:     88 '<unused81>' is not marked as EOG\n",
      "load: control token:      0 '<pad>' is not marked as EOG\n",
      "load: control token:     58 '<unused51>' is not marked as EOG\n",
      "load: control token:     67 '<unused60>' is not marked as EOG\n",
      "load: control token:     22 '<unused15>' is not marked as EOG\n",
      "load: control token:     14 '<unused7>' is not marked as EOG\n",
      "load: control token:     31 '<unused24>' is not marked as EOG\n",
      "load: control token:    101 '<unused94>' is not marked as EOG\n",
      "load: control token:      2 '<bos>' is not marked as EOG\n",
      "load: control token:     27 '<unused20>' is not marked as EOG\n",
      "load: control token:     66 '<unused59>' is not marked as EOG\n",
      "load: control token:      7 '<unused0>' is not marked as EOG\n",
      "load: control token:     98 '<unused91>' is not marked as EOG\n",
      "load: control token:     25 '<unused18>' is not marked as EOG\n",
      "load: control token:     61 '<unused54>' is not marked as EOG\n",
      "load: control token:     72 '<unused65>' is not marked as EOG\n",
      "load: control token:     29 '<unused22>' is not marked as EOG\n",
      "load: control token:     36 '<unused29>' is not marked as EOG\n",
      "load: control token:      3 '<unk>' is not marked as EOG\n",
      "load: control token:     45 '<unused38>' is not marked as EOG\n",
      "load: control token:      4 '<mask>' is not marked as EOG\n",
      "load: control token:     83 '<unused76>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: printing all EOG tokens:\n",
      "load:   - 1 ('<eos>')\n",
      "load:   - 107 ('<end_of_turn>')\n",
      "load: special tokens cache size = 249\n",
      "load: token to piece cache size = 1.6014 MB\n",
      "print_info: arch             = gemma2\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 8192\n",
      "print_info: n_embd           = 2304\n",
      "print_info: n_layer          = 26\n",
      "print_info: n_head           = 8\n",
      "print_info: n_head_kv        = 4\n",
      "print_info: n_rot            = 256\n",
      "print_info: n_swa            = 4096\n",
      "print_info: is_swa_any       = 1\n",
      "print_info: n_embd_head_k    = 256\n",
      "print_info: n_embd_head_v    = 256\n",
      "print_info: n_gqa            = 2\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 6.2e-02\n",
      "print_info: n_ff             = 9216\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 2\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 8192\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 2B\n",
      "print_info: model params     = 2.61 B\n",
      "print_info: general.name     = Gemma 2 2b Jpn It\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 256000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 2 '<bos>'\n",
      "print_info: EOS token        = 1 '<eos>'\n",
      "print_info: EOT token        = 107 '<end_of_turn>'\n",
      "print_info: UNK token        = 3 '<unk>'\n",
      "print_info: PAD token        = 0 '<pad>'\n",
      "print_info: LF token         = 227 '<0x0A>'\n",
      "print_info: EOG token        = 1 '<eos>'\n",
      "print_info: EOG token        = 107 '<end_of_turn>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q6_K) (and 288 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
      "load_tensors: offloading 0 repeating layers to GPU\n",
      "load_tensors: offloaded 0/27 layers to GPU\n",
      "load_tensors:   CPU_Mapped model buffer size =  1323.24 MiB\n",
      "...................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 4096\n",
      "llama_context: n_ctx_per_seq = 4096\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: kv_unified    = false\n",
      "llama_context: freq_base     = 10000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M3\n",
      "ggml_metal_init: picking default device: Apple M3\n",
      "ggml_metal_init: GPU name:   Apple M3\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "ggml_metal_init: loaded kernel_add                                    0x1602c78e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_2                             0x130271a50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_3                             0x130271eb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_4                             0x1302728e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_5                             0x1602c7cc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_6                             0x1602c8270 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_7                             0x1602c8530 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_8                             0x130272c80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4                             0x1602c8830 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_2                      0x1602c8c10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_3                      0x1602c9080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_4                      0x1602c9630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_5                      0x130272fe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_6                      0x1078da130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_7                      0x1078da3f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_8                      0x1602c98f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub                                    0x1078a6a40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub_row_c4                             0x1602c9bb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                                    0x1602ca120 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row_c4                             0x1078a6ee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div                                    0x1078a71a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div_row_c4                             0x1078a74b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_id                                 0x1602ca4e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f32                             0x1602ca7e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f16                             0x1602cabb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i32                             0x1302733e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i16                             0x1078a78b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                                  0x1078a7c80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale_4                                0x1602caf80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_clamp                                  0x130273920 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_tanh                                   0x1602cb360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                                   0x1078a80a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sigmoid                                0x15ffdb330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                                   0x1078a8400 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_4                                 0x15ffdb5f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_erf                               0x1078a8810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_erf_4                             0x1078e6b70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick                             0x1078a8b90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                           0x15ffdb8b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                                   0x15ffdbdd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu_4                                 0x15ffdc2a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_elu                                    0x1078e4080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_abs                                    0x15ffdc560 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sgn                                    0x131877810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_step                                   0x1078e4640 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_hardswish                              0x1078e4900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_hardsigmoid                            0x1078e4cc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_exp                                    0x1078e4ff0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16                           0x1078e53c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1078e5790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32                           0x1602cba40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15ffdc820 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                          0x15ffdcb20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1078e5b60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                           0x1602cbec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                           0x1078e5ec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1078e6290 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1078e6660 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15ffdce80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1078dc260 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1078dc570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_mxfp4                         0x15ffdd250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1078a2f90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15ffdd620 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1078a3250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1602cc180 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15ffddd20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15ffddfe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1078792a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15ffde2a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1602cc5e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x107879560 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15ffde880 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1602cc9f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15ffded50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x107879820 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                           0x1602ccd90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_f32                           0x107879b80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_f16                           0x1602cd050 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_set_rows_q8_0                          0x15ffdf080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q4_0                          0x107879fe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q4_1                          0x1602cd520 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q5_0                          0x15ffdf340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q5_1                          0x1602cd7e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_iq4_nl                        0x10787a380 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                               0x15ffdf810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm_mul                           0x10787a6e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm_mul_add                       0x1602cdd20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_l2_norm                                0x15ffdfc20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_group_norm                             0x1602ce0c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                                   0x15ffdffc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15ffe0280 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1602ce380 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32_group                     0x15ffe05e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rwkv_wkv6_f32                          0x1602ce640 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rwkv_wkv7_f32                          0x15ffe0b40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10787aab0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32_c4                      0x1602ceaa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15ffe0ee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_c4                      0x1602ced60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1602cf1c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10787ae80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1602cf590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10787b2e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1602cf8f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1602cfc70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10787b6b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10787ba10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_mxfp4_f32                       0x1602d0040 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10787bde0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15ffe11a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10787c1b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10787c610 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10787c9e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15ffe1620 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10787cd40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15ffe18e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15ffe1ba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15ffe1fb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10787d1a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10787d570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15ffe2310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15ffe26e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1602d0410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10787d8d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15ffe2a40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1602d09a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10787dca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10787e100 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10787e4d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10787e830 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1602d0c60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10787ec00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_2              0x10787efd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_3              0x15ffe2e10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_4              0x1602d0fb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_5              0x10787f3a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15ffe31e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1602d1380 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1602d17f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1602d1bc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10787f770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1602d2160 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1602d2420 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1602d2720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15ffe35c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1602d2af0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15ffe3990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10787fb40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10787ffa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15ffe3d80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x107880300 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1602d3150 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1602d3410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15ffe4150 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1078806d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1602d3940 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1602d3c00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x107880aa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1602d3ec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1602d4220 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1602d4580 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x107880e70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15ffe4520 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15ffe4910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15ffe4d80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x107881240 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15ffe50e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x107881610 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x107881a70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1602d4900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x107881dd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1602d4ce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15ffe57a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_mxfp4_f32                    0x15ffe5a60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x107882230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1602d50b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1602d5510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x107882590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1602d5870 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15ffe5f30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x107882960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1602d5c40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1602d62c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15ffe6400 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x107882dc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x107883190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1602d6720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1602d69e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1078834f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15ffe66c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1602d6ca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x107883a30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15ffe6ad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x107883cf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1602d6ff0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_mxfp4_f32                       0x1602d73c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_mxfp4_f32                       0x15ffe6e50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15ffe7160 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15ffe7420 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x107884100 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15ffe77c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1078844d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15ffe7b20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15ffe8190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15ffe8450 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x107885fd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15ffe8990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15ffe8c50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15ffe8f10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15ffe9240 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1078863a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_map0_f16                     0x15ffe9810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_map1_f32                     0x107886800 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f32_f16                      0x1602d7720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f16_f16                      0x107886b60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f16                     0x107887000 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f16                     0x1602d7af0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f16                     0x107887360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f16                     0x15ffe9ad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f16                     0x1602d80a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_mxfp4_f16                    0x107887850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f16                     0x15ffe9ea0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f16                     0x15ffea1b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f16                     0x1602d85d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f16                     0x15ffea510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f16                     0x15ffea870 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f16                  0x15ffeaf90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f16                   0x1602d8890 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f16                  0x15ffeb250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f16                    0x1602d8ba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f16                    0x107887b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f16                    0x1602d94a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f16                    0x107887f10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f16                   0x1602d8f40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f16                   0x1078c1f00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                          0x1078c22c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                          0x1602d9760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_multi_f32                         0x1078c2620 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_multi_f16                         0x13191cad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_vision_f32                        0x131e81cd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_vision_f16                        0x1078c2970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                          0x1078c2dd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                          0x131e81f90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f16                             0x1078c3130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f32                             0x131e82250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1078c3590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15c3821d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1078c38f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x131877ad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_upscale_f32                            0x1078eac50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_f32                                0x1078eaf10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1602d9a20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x131877f70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_arange_f32                             0x1078e3930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1602d9e30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1602da1d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1078e3c90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1078e0540 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1078e08a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1078e0cd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x130618d70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1602da860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h192                0x1602dab20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk192_hv128         0x1078e0f90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1078e1360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk576_hv512         0x1602dade0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15ffeb5b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1078e1730 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1078e1b90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1078e1f60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1078e2330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h192               0x15ffeb870 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk192_hv128        0x15ffebc10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15ffebfe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk576_hv512        0x15ffec3b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1078e2700 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15ffec780 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15ffecb50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1078e2b60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1078e2eb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h192               0x1078e3280 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk192_hv128        0x1078e85b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15ffecfb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk576_hv512        0x1078e8870 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1078e8c30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15ffed4c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15ffed780 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15ffedab0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15ffede80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h192               0x15ffee250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk192_hv128        0x15c382490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1078e8ef0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk576_hv512        0x15ffee620 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15ffee9f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15ffeefe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15ffef2a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1602db1b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15ffef560 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h192               0x15ffef970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk192_hv128        0x15ffefcd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15fff00f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk576_hv512        0x1078e9390 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1078e9650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1078e9a20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x131a42e00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x131a43230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x130619140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h192               0x15fff0450 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk192_hv128        0x1602db510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1602db8f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk576_hv512        0x15fff0a60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h64             0x1602dbcd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h64            0x15fff0f00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h64            0x15fff11c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h64            0x15fff14d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h64            0x1602dc0a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h64            0x1078e9df0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h96             0x15fff17e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h96            0x1602dc470 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h96            0x1078ea250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h96            0x15fff1b80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h96            0x15fff1fe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h96            0x1078ea5b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1078d7600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1078d78c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15fff2340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15fff2710 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1078d7b80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1602dc850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h192            0x1078d7f00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h192           0x15fff2b70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h192           0x1078d82d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h192           0x1078d8910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h192           0x1078d8bd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h192           0x1078d8ed0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk192_hv128      0x1602dcc20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk192_hv128      0x15fff2ed0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk192_hv128      0x131878230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk192_hv128      0x13191e190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk192_hv128      0x1078d4ca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk192_hv128      0x1602dd080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1078d5070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1602dd3e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1078d5440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1078d5810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1078d5be0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1602dd840 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk576_hv512      0x1602ddc10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk576_hv512      0x1602ddf70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk576_hv512      0x1602de3d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk576_hv512      0x15fff3400 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk576_hv512      0x1318785d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk576_hv512      0x13191e620 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_f32                                0x1602de730 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_i32                                0x13191e9e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1078d5f40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15fff3700 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15fff3b60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1078d6310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15fff3eb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1078d6770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1078d6b40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1078d6f10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15fff43c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1602deb30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x15fff4680 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x15c3828a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x1602deed0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x15c382bb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x1602df2b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x15fff49b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x1602df690 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x1602dfaf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x15c382f50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x15c383320 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_concat                                 0x1602dfe50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqr                                    0x15fff4e50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqrt                                   0x1602e0230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sin                                    0x15fff53a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cos                                    0x15c383700 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_neg                                    0x1602e0600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_reglu                                  0x1602e0a60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu                                  0x1602e0e30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_swiglu                                 0x15fff5660 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_swiglu_oai                             0x1602e1190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu_erf                              0x1602e15f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu_quick                            0x1602e1b30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sum_rows                               0x1602e1e90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mean                                   0x15fff59b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argmax                                 0x15fff5d80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1602e2150 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1602e2560 | th_max = 1024 | th_width =   32\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.98 MiB\n",
      "create_memory: n_ctx = 4096 (padded)\n",
      "llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)\n",
      "llama_kv_cache_unified_iswa: creating non-SWA KV cache, size = 4096 cells\n",
      "llama_kv_cache_unified: layer   0: skipped\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: skipped\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: skipped\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: skipped\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: skipped\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: skipped\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: skipped\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: skipped\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: skipped\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: skipped\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: skipped\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: skipped\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified: layer  24: skipped\n",
      "llama_kv_cache_unified: layer  25: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =   208.00 MiB\n",
      "llama_kv_cache_unified: size =  208.00 MiB (  4096 cells,  13 layers,  1/1 seqs), K (f16):  104.00 MiB, V (f16):  104.00 MiB\n",
      "llama_kv_cache_unified_iswa: creating     SWA KV cache, size = 4096 cells\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: skipped\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: skipped\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: skipped\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: skipped\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: skipped\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: skipped\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: skipped\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: skipped\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: skipped\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: skipped\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: skipped\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: skipped\n",
      "llama_kv_cache_unified: layer  24: dev = CPU\n",
      "llama_kv_cache_unified: layer  25: skipped\n",
      "llama_kv_cache_unified:        CPU KV buffer size =   208.00 MiB\n",
      "llama_kv_cache_unified: size =  208.00 MiB (  4096 cells,  13 layers,  1/1 seqs), K (f16):  104.00 MiB, V (f16):  104.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 3\n",
      "llama_context: max_nodes = 2304\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "llama_context:        CPU compute buffer size =   504.50 MiB\n",
      "llama_context: graph nodes  = 1128\n",
      "llama_context: graph splits = 367 (with bs=512), 1 (with bs=1)\n",
      "Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | REPACK = 1 | \n",
      "Model metadata: {'quantize.imatrix.entries_count': '182', 'quantize.imatrix.file': 'imatrix.dat', 'quantize.imatrix.chunks_count': '636', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.eos_token_id': '1', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'general.type': 'model', 'tokenizer.ggml.bos_token_id': '2', 'general.quantization_version': '2', 'gemma2.attention.value_length': '256', 'gemma2.attention.sliding_window': '4096', 'tokenizer.ggml.pre': 'default', 'tokenizer.ggml.unknown_token_id': '3', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '4', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.head_count': '8', 'tokenizer.ggml.model': 'llama', 'gemma2.feed_forward_length': '9216', 'gemma2.context_length': '8192', 'general.organization': 'Google', 'gemma2.block_count': '26', 'tokenizer.ggml.add_space_prefix': 'false', 'general.architecture': 'gemma2', 'general.basename': 'gemma-2', 'gemma2.embedding_length': '2304', 'general.size_label': '2B', 'gemma2.attention.key_length': '256', 'general.file_type': '27', 'general.finetune': 'jpn-it', 'quantize.imatrix.dataset': '.\\\\calibration_datav3_plus_jpn_v1.txt', 'general.name': 'Gemma 2 2b Jpn It'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "llama_perf_context_print:        load time =    1537.66 ms\n",
      "llama_perf_context_print: prompt eval time =    1537.46 ms /     5 tokens (  307.49 ms per token,     3.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2901.87 ms /    15 runs   (  193.46 ms per token,     5.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    4457.99 ms /    20 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-b4617e5d-5677-4dce-86a2-8c6aca07d511', 'object': 'text_completion', 'created': 1758326202, 'model': '/Users/mori/.cache/huggingface/hub/models--webbigdata--gemma-2-2b-jpn-it-translate-gguf/snapshots/e9c0c0452c774b1528768ffe0980968f116196dc/./gemma-2-2b-jpn-it-translate-IQ3_M.gguf', 'choices': [{'text': \" 🌎\\n\\nThis is a simple blog post about something I'm passionate about\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 5, 'completion_tokens': 16, 'total_tokens': 21}}\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "repo_id: str = \"webbigdata/gemma-2-2b-jpn-it-translate-gguf\"\n",
    "filename: str = \"gemma-2-2b-jpn-it-translate-IQ3_M.gguf\"\n",
    "\n",
    "llm = Llama.from_pretrained(\n",
    "    repo_id=repo_id,\n",
    "    filename=filename,\n",
    "    n_ctx=4096,\n",
    "    n_threads=4,\n",
    "    n_gpu_layers=0,\n",
    "    temperature=0.2,\n",
    "    top_p=0.95,\n",
    "    max_tokens=2048,\n",
    "    system_prompt=\"以下の英文を日本語に翻訳し、翻訳した結果のみを出力してください。 例：(入力)This is a test. -> (出力)これはテストです。\"\n",
    ")\n",
    "\n",
    "print(llm(\"Hello, world!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db304014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<feed xmlns=\"http://www.w3.org/2005/Atom\">\n",
      "  <link href=\"http://arxiv.org/api/query?search_query%3Dti%3A%22language%20model%22%20AND%20abs%3A%22distillation%22%26id_list%3D%26start%3D0%26max_results%3D5\" rel=\"self\" type=\"application/atom+xml\"/>\n",
      "  <title type=\"html\">ArXiv Query: search_query=ti:\"language model\" AND abs:\"distillation\"&amp;id_list=&amp;start=0&amp;max_results=5</title>\n",
      "  <id>http://arxiv.org/api/TfsaVs9T7Qi+bmYuluq9IETuM+Q</id>\n",
      "  <updated>2025-09-19T00:00:00-04:00</updated>\n",
      "  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">759</opensearch:totalResults>\n",
      "  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\n",
      "  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">5</opensearch:itemsPerPage>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2509.14930v1</id>\n",
      "    <updated>2025-09-18T13:07:53Z</updated>\n",
      "    <published>2025-09-18T13:07:53Z</published>\n",
      "    <title>Cross-Modal Knowledge Distillation for Speech Large Language Models</title>\n",
      "    <summary>  In this work, we present the first systematic evaluation of catastrophic\n",
      "forgetting and modality inequivalence in speech large language models, showing\n",
      "that introducing speech capabilities can degrade knowledge and reasoning even\n",
      "when inputs remain textual, and performance further decreases with spoken\n",
      "queries. To address these challenges, we propose a cross-modal knowledge\n",
      "distillation framework that leverages both text-to-text and speech-to-text\n",
      "channels to transfer knowledge from a text-based teacher model to a speech LLM.\n",
      "Extensive experiments on dialogue and audio understanding tasks validate the\n",
      "effectiveness of our approach in preserving textual knowledge, improving\n",
      "cross-modal alignment, and enhancing reasoning in speech-based interactions.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Enzhi Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Qicheng Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhiyuan Tang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yuhang Jia</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/2509.14930v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2509.14930v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2509.14526v1</id>\n",
      "    <updated>2025-09-18T01:42:24Z</updated>\n",
      "    <published>2025-09-18T01:42:24Z</published>\n",
      "    <title>Delta Knowledge Distillation for Large Language Models</title>\n",
      "    <summary>  Knowledge distillation (KD) is a widely adopted approach for compressing\n",
      "large neural networks by transferring knowledge from a large teacher model to a\n",
      "smaller student model. In the context of large language models, token level KD,\n",
      "typically minimizing the KL divergence between student output distribution and\n",
      "teacher output distribution, has shown strong empirical performance. However,\n",
      "prior work assumes student output distribution and teacher output distribution\n",
      "share the same optimal representation space, a premise that may not hold in\n",
      "many cases. To solve this problem, we propose Delta Knowledge Distillation\n",
      "(Delta-KD), a novel extension of token level KD that encourages the student to\n",
      "approximate an optimal representation space by explicitly preserving the\n",
      "distributional shift Delta introduced during the teacher's supervised\n",
      "finetuning (SFT). Empirical results on ROUGE metrics demonstrate that Delta KD\n",
      "substantially improves student performance while preserving more of the\n",
      "teacher's knowledge.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Yihan Cao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yanbin Kang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhengming Xing</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ruijie Jiang</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8 pages, 3 figures</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/2509.14526v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2509.14526v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2509.13785v1</id>\n",
      "    <updated>2025-09-17T07:55:39Z</updated>\n",
      "    <published>2025-09-17T07:55:39Z</published>\n",
      "    <title>Summary on The Multilingual Conversational Speech Language Model\n",
      "  Challenge: Datasets, Tasks, Baselines, and Methods</title>\n",
      "    <summary>  This paper summarizes the Interspeech2025 Multilingual Conversational Speech\n",
      "Language Model (MLC-SLM) challenge, which aims to advance the exploration of\n",
      "building effective multilingual conversational speech LLMs (SLLMs). We provide\n",
      "a detailed description of the task settings for the MLC-SLM challenge, the\n",
      "released real-world multilingual conversational speech dataset totaling\n",
      "approximately 1,604 hours, and the baseline systems for participants. The\n",
      "MLC-SLM challenge attracts 78 teams from 13 countries to participate, with 489\n",
      "valid leaderboard results and 14 technical reports for the two tasks. We\n",
      "distill valuable insights on building multilingual conversational SLLMs based\n",
      "on submissions from participants, aiming to contribute to the advancement of\n",
      "the community.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Bingshen Mu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Pengcheng Guo</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhaokai Sun</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shuai Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hexin Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mingchen Shao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Lei Xie</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Eng Siong Chng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Longshuai Xiao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Qiangze Feng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Daliang Wang</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/2509.13785v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2509.13785v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2509.11815v1</id>\n",
      "    <updated>2025-09-15T11:53:56Z</updated>\n",
      "    <published>2025-09-15T11:53:56Z</published>\n",
      "    <title>SpecVLM: Fast Speculative Decoding in Vision-Language Models</title>\n",
      "    <summary>  Speculative decoding is a powerful way to accelerate autoregressive large\n",
      "language models (LLMs), but directly porting it to vision-language models\n",
      "(VLMs) faces unique systems constraints: the prefill stage is dominated by\n",
      "visual tokens whose count scales with image resolution and video length,\n",
      "inflating both compute and memory, especially the key-value (KV) cache. We\n",
      "study speculative decoding for VLMs and introduce SpecVLM, a practical system\n",
      "that (1) establishes a strong EAGLE-2-style baseline, EagleVLM, delivering\n",
      "1.5--2.3x end-to-end speedups over full autoregressive inference, and (2)\n",
      "further accelerates VLM inference with an elastic visual compressor that\n",
      "adaptively selects among pruning, pooling, convolution, and resampler\n",
      "primitives to balance FLOPs/parameters and accuracy per input. To avoid costly\n",
      "offline distillation corpora, we propose an online-logit distillation protocol\n",
      "that trains the draft model with on-the-fly teacher logits and penultimate\n",
      "features using a combined cross-entropy and Smooth L1 objective, eliminating\n",
      "storage and preprocessing while remaining compute-efficient. This protocol\n",
      "reveals a training-time scaling effect: longer online training monotonically\n",
      "increases the draft model's average accepted length, improving speculative\n",
      "efficiency. Empirically, SpecVLM achieves additional acceleration, culminating\n",
      "in 2.5--2.9x end-to-end speedups within 5 epochs across LLaVA and MMMU,\n",
      "consistently over resolutions and task difficulties, while preserving the\n",
      "target model's output distribution (lossless decoding). Our code is available\n",
      "at https://github.com/haiduo/SpecVLM.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Haiduo Huang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Fuwei Yang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhenhua Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xuanwu Yin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Dong Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Pengju Ren</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Emad Barsoum</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/2509.11815v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2509.11815v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2509.11575v1</id>\n",
      "    <updated>2025-09-15T04:39:50Z</updated>\n",
      "    <published>2025-09-15T04:39:50Z</published>\n",
      "    <title>A Survey of Reasoning and Agentic Systems in Time Series with Large\n",
      "  Language Models</title>\n",
      "    <summary>  Time series reasoning treats time as a first-class axis and incorporates\n",
      "intermediate evidence directly into the answer. This survey defines the problem\n",
      "and organizes the literature by reasoning topology with three families: direct\n",
      "reasoning in one step, linear chain reasoning with explicit intermediates, and\n",
      "branch-structured reasoning that explores, revises, and aggregates. The\n",
      "topology is crossed with the main objectives of the field, including\n",
      "traditional time series analysis, explanation and understanding, causal\n",
      "inference and decision making, and time series generation, while a compact tag\n",
      "set spans these axes and captures decomposition and verification, ensembling,\n",
      "tool use, knowledge access, multimodality, agent loops, and LLM alignment\n",
      "regimes. Methods and systems are reviewed across domains, showing what each\n",
      "topology enables and where it breaks down in faithfulness or robustness, along\n",
      "with curated datasets, benchmarks, and resources that support study and\n",
      "deployment (https://github.com/blacksnail789521/Time-Series-Reasoning-Survey).\n",
      "Evaluation practices that keep evidence visible and temporally aligned are\n",
      "highlighted, and guidance is distilled on matching topology to uncertainty,\n",
      "grounding with observable artifacts, planning for shift and streaming, and\n",
      "treating cost and latency as design budgets. We emphasize that reasoning\n",
      "structures must balance capacity for grounding and self-correction against\n",
      "computational cost and reproducibility, while future progress will likely\n",
      "depend on benchmarks that tie reasoning quality to utility and on closed-loop\n",
      "testbeds that trade off cost and risk under shift-aware, streaming, and\n",
      "long-horizon settings. Taken together, these directions mark a shift from\n",
      "narrow accuracy toward reliability at scale, enabling systems that not only\n",
      "analyze but also understand, explain, and act on dynamic worlds with traceable\n",
      "evidence and credible outcomes.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Ching Chang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yidan Shi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Defu Cao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Wei Yang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jeehyun Hwang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Haixin Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jiacheng Pang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Wei Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yan Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Wen-Chih Peng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tien-Fu Chen</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">This paper is currently under review</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/2509.11575v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2509.11575v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "</feed>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "base_url = \"http://export.arxiv.org/api/query\"\n",
    "params = {\n",
    "    \"search_query\": 'ti:\"language model\" AND abs:\"distillation\"',\n",
    "    \"sortBy\": \"submittedDate\",\n",
    "    \"sortOrder\": \"descending\",\n",
    "    \"max_results\": 5\n",
    "}\n",
    "\n",
    "response = requests.get(base_url, params=params)\n",
    "print(response.text)  # XMLレスポンス\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
